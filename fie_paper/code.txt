#!/usr/bin/env python
# test_bandit_minimal.py
# Minimal test of the contextual bandit implementation

import numpy as np
from sklearn.linear_model import BayesianRidge


# Contextual Thompson Sampling class (fixed version)
class CTBandit:
    def __init__(self, n_arms, n_feats):
        # Create separate model instances for each arm
        self.models = [BayesianRidge() for _ in range(n_arms)]
        self.buffers = [list() for _ in range(n_arms)]
        self.trained = [False] * n_arms
        self.n_arms = n_arms
        print(f"Initialized bandit with {n_arms} arms and {n_feats} features")

    def act(self, x):
        """Select action using Thompson Sampling"""
        theta = np.zeros(self.n_arms)

        for a in range(self.n_arms):
            if self.trained[a]:
                mu, sigma = self.models[a].predict(x.reshape(1, -1), return_std=True)
                mu_val = mu.item()
                sigma_val = sigma.item()
                theta[a] = np.random.normal(mu_val, max(sigma_val, 1e-6))
            else:
                theta[a] = 0.75  # Optimistic prior

        chosen_arm = int(theta.argmax())
        print(f"  Theta values: {theta}, Chosen arm: {chosen_arm}")
        return chosen_arm

    def learn(self, a, x, r):
        """Update model for arm a with context x and reward r"""
        self.buffers[a].append((x, r))
        print(f"  Learning: arm={a}, reward={r}, buffer_size={len(self.buffers[a])}")

        if len(self.buffers[a]) >= 10 and len(self.buffers[a]) % 10 == 0:
            Xbuf, rbuf = zip(*self.buffers[a])
            self.models[a] = BayesianRidge()
            self.models[a].fit(np.vstack(Xbuf), np.array(rbuf))
            self.trained[a] = True
            print(f"  âœ“ Retrained model for arm {a}")


# Test the bandit with synthetic data
print("Testing Contextual Thompson Sampling Bandit\n")

# Create synthetic problem
n_arms = 3
n_features = 5
n_samples = 50

# Initialize bandit
bandit = CTBandit(n_arms=n_arms, n_feats=n_features)

# Generate synthetic data
np.random.seed(42)
X_test = np.random.randn(n_samples, n_features)


# True reward function: arm 0 is best for negative feature 0,
# arm 1 is best for positive feature 0, arm 2 is random
def true_reward(x, arm):
    if arm == 0:
        return 0.7 + 0.2 * (x[0] < 0)  # Good when feature 0 is negative
    elif arm == 1:
        return 0.7 + 0.2 * (x[0] > 0)  # Good when feature 0 is positive
    else:
        return 0.5 + 0.1 * np.random.randn()  # Random baseline


# Run simulation
print("\nRunning simulation...")
rewards = []

for t in range(n_samples):
    print(f"\nStep {t+1}:")
    x = X_test[t]

    # Bandit chooses action
    action = bandit.act(x)

    # Get reward
    reward = true_reward(x, action)
    rewards.append(reward)

    # Update bandit
    bandit.learn(action, x, reward)

# Print summary
print(f"\n{'='*50}")
print(f"Simulation complete!")
print(f"Average reward: {np.mean(rewards):.3f}")
print(f"Last 10 rewards: {np.mean(rewards[-10:]):.3f}")

# Check which arms were trained
print(f"\nArms trained: {[i for i, trained in enumerate(bandit.trained) if trained]}")
